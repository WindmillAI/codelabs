{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WindmillAI 5 minute start",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHw523cE8bST"
      },
      "source": [
        "# WindmillAI 5 minute start\n",
        "\n",
        "In this codelab we'll train a toy nonlinear regression\n",
        "using [WindmillAI](http://www.WindmillAI.com) as the\n",
        "experiment orchestrator. If you're not familiar with Google Colab just follow the instructions and click the play button on the left side of each code cell to move down the code lab. If you're viewing this notebook on Github and not Google Colab then [open it in Colab](https://colab.research.google.com/githubWindmillAI/codelabs/blob/main/codelabs/quickstart/notebook.ipynb).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA6eYP6u89Tn"
      },
      "source": [
        "As a first step we'll install and import the [WindmillAI client](https://github.com/windmillAI/windmillaipy) and some common machine learning libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSIj-UPP8XHY"
      },
      "source": [
        "!pip install git+https://github.com/WindmillAI/windmillaipy.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijmClYXR9Po8"
      },
      "source": [
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as pl\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import windmillaipy.client as wm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsWHTUE6xQ2G"
      },
      "source": [
        "For the next step you'll need to have signed up for [WindmillAI](http://www.WindmillAI.com) service. Just click the sign up button on the top right corner. Don't worry, it's free and only takes a few seconds.\n",
        "\n",
        "Once you have signed up you'll need to retrieve an API key from [your profile page](https://www.windmillai.com/profile). API keys are secrets specific to you, and you can create and delete them on your profile page as needed.\n",
        "\n",
        "In your code you can manage your secrets normally, or export your key as the WINDMILLAI_API_KEY environment variable and the client will pick that up automatically if you initialize it with no arguments.\n",
        "\n",
        "Enter your API key in the text box below and then create a client associated to your account:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "20AO805TxrLP"
      },
      "source": [
        "API_KEY = ''  # @param{type: 'string'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRftejzqwMWT"
      },
      "source": [
        "client: wm.WindmillClient = wm.WindmillClient(api_key=API_KEY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS7QqChYzc8K"
      },
      "source": [
        "In this next block of code we're going to write a data generator and a tiny deep neural network that learns a sine wave from noisy data. You can skip over this if you're just interested in the WindmillAI-specific parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6szybkbzu7H"
      },
      "source": [
        "def data_generator(num_samples: int = 512):\n",
        "  'Generate (x, y) data.'\n",
        "  \n",
        "  x = np.random.uniform(0, 25, size=num_samples)\n",
        "  y = np.sin(x) + np.random.normal(scale=0.5, size=num_samples)\n",
        "  \n",
        "  return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CdRogGysz9O"
      },
      "source": [
        "def plot_result(x, y, regression=None, fileish=None):\n",
        "  'Plot (x, y) data samples and overlay regression (if available).'\n",
        "\n",
        "  pl.figure(figsize=(10, 5))\n",
        "\n",
        "  if regression:\n",
        "    xr = np.linspace(0, 25, 51)\n",
        "    xr = np.expand_dims(xr, axis=1)\n",
        "    yr = regression(xr)\n",
        "    xr = np.squeeze(xr)\n",
        "    yr = np.squeeze(yr)\n",
        "\n",
        "  pl.subplot(1, 2, 1)\n",
        "  pl.scatter(x, y)\n",
        "  if regression:\n",
        "    pl.plot(xr, yr, color='red', linewidth=8)\n",
        "\n",
        "  pl.subplot(1, 2, 2)\n",
        "  pl.hexbin(x, y, gridsize=20)\n",
        "  if regression:\n",
        "    pl.plot(xr, yr, color='red', linewidth=8)\n",
        "\n",
        "  if fileish:\n",
        "    pl.savefig(fileish, format='png')\n",
        "  else:\n",
        "    pl.show()\n",
        "\n",
        "  pl.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZvc9NjG0j08"
      },
      "source": [
        "x, y = data_generator(1000)\n",
        "\n",
        "plot_result(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpOkswpH1CtD"
      },
      "source": [
        "class Learner(tf.keras.Model):\n",
        "  def __init__(self, network_width=25, num_hidden_layers=1):\n",
        "    super(Learner, self).__init__()\n",
        "\n",
        "    self.hidden_layers = []\n",
        "    for _ in range(num_hidden_layers):\n",
        "      self.hidden_layers.append(tf.keras.layers.Dense(units=network_width, activation=tf.nn.relu))\n",
        "\n",
        "    self.out = tf.keras.layers.Dense(units=1, activation=None)\n",
        "\n",
        "  def call(self, x):\n",
        "    net = np.array([x], ndmin=2)\n",
        "    for layer in self.hidden_layers:\n",
        "      net = layer(net)\n",
        "    return self.out(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBhtlPI4iobw"
      },
      "source": [
        "We'll now create an experiment on WindmillAI. You'll be able to monitor and archive training progress below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIxvKwK7inXD"
      },
      "source": [
        "wu: wm.WorkUnit = client.create_experiment('quickstart-codelab',\n",
        "                                           tags=['codelab', 'quickstart'])\n",
        "\n",
        "print(f'Experiment page at http://www.windmillai.com/experiment/{wu.xid}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRpsCmcZqELE"
      },
      "source": [
        "This part contains the training loop. This may take a few minutes to run. Your experiment progress will be reported to Windmill. Use the link above to see what Windmill is archiving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FgEfuWaqDFZ"
      },
      "source": [
        "NUM_EPOCHS = 100000\n",
        "\n",
        "learner = Learner()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "wu.add_diary_entry('Learning sin(x) + N(0, sigma)')\n",
        "\n",
        "for epoch in tqdm.tqdm(range(NUM_EPOCHS)):\n",
        "  xs, ys = data_generator()\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    xs = np.expand_dims(xs, axis=1)\n",
        "    ys = np.expand_dims(ys, axis=1)\n",
        "    yp = learner(xs, training=True)\n",
        "    loss = tf.reduce_mean(tf.square(ys - yp))\n",
        "    gradients = tape.gradient(loss, learner.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, learner.trainable_variables))\n",
        "\n",
        "  if epoch % 1000 == 0 or epoch == NUM_EPOCHS-1:\n",
        "    wu.record_measurements([{\n",
        "        'label': 'loss',\n",
        "        'steps': epoch,\n",
        "        'value': float(np.mean(loss)),\n",
        "        }])\n",
        "\n",
        "  if epoch % 10000 == 0 or epoch == NUM_EPOCHS-1:\n",
        "    img = io.BytesIO()\n",
        "    plot_result(x, y, learner, img)\n",
        "    wu.create_artifact(f'regression_at_{epoch}.png', img.getvalue())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaQpLlAEQqXN"
      },
      "source": [
        "We can quickly plot the results here, but you can also see this plot (and similar throughout training) on the WindmillAI experiment page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuOlPBIPs6d4"
      },
      "source": [
        "plot_result(x, y, learner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92mc9LxRQ74x"
      },
      "source": [
        "Finally we mark the experiment complete. If we wanted to save this model we would checkpoint the graph and upload it as an artifact first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmMbmXgdJISF"
      },
      "source": [
        "wu.complete_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXk4uJT9Q-ub"
      },
      "source": [
        "Typically we wouldn't use this type of instrumentation on a Python notebook, but this illustrates the general flow of how Windmill can augment your ML code so you can focus on the models and not how you're going to keep track of data and observe the progress of jobs running in the cloud."
      ]
    }
  ]
}